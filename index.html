<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Tianyang Liu</title> <meta name="author" content="Tianyang Liu"> <meta name="description" content="Tianyang Liu's personal website. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:wght@300;400;450;500;600;700&amp;display=swap" rel="stylesheet"> <script src="https://code.iconify.design/iconify-icon/1.0.7/iconify-icon.min.js"></script> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400..800;1,400..800&amp;display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&amp;family=EB+Garamond:ital,wght@0,400..800;1,400..800&amp;display=swap" rel="stylesheet"> <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://leoii22.com/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?59753a4dbd0cf61300e258426597872f"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.3.3/dist/echarts.min.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="https://scholar.google.com/citations?user=rJAeYdwAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-google"></i></a> <a href="https://www.semanticscholar.org/author/2115347044" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a href="https://github.com/leolty" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github-alt"></i></a> <a href="https://www.linkedin.com/in/tianyangliu-whu-ucsd" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin-in"></i></a> <a href="https://twitter.com/ltyleoii22" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> <a href="https://instagram.com/leoii22" title="Instagram" rel="external nofollow noopener" target="_blank"><i class="fab fa-instagram"></i></a> <a href="/assets/pdf/TianyangLiu_CV_Jan_2025.pdf" title="CV"><i class="fas fa-file-pdf"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <link rel="preload" as="image" href="/assets/img/me_1.jpg"> <link rel="preload" as="image" href="/assets/img/me_2.jpg"> <link rel="preload" as="image" href="/assets/img/me_3.jpg"> <link rel="preload" as="image" href="/assets/img/me_4.jpg"> <link rel="preload" as="image" href="/assets/img/me_5.jpg"> <link rel="preload" as="image" href="/assets/img/me_6.jpg"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Tianyang</span> Liu </h1> <p class="desc">Ph.D. Student at UC San Diego <br> <span style="white-space: nowrap; cursor: pointer; font-family: 'Courier New', monospace; font-size: 13px; transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1); position: relative; display: inline-flex; align-items: center; gap: 4px; padding: 2px 4px; border-radius: 3px; user-select: none;" onmousemove=" const bounds=this.getBoundingClientRect(); const x=(event.clientX - bounds.left) / bounds.width; const y=(event.clientY - bounds.top) / bounds.height; const brackets=this.querySelectorAll('.bracket'); brackets.forEach(b =&gt; { const bx=(b.getBoundingClientRect().left - bounds.left) / bounds.width; const distance=Math.abs(x - bx); const strength=Math.max(0, 1 - distance * 2); const moveX=(x - 0.5) * 4; const moveY=(y - 0.5) * 2; b.style.transform = `translate(${moveX * strength}px, ${moveY * strength}px) scale(${1 + strength * 0.1})`; b.style.opacity = 0.7 + (strength * 0.3); });" onmouseover="this.style.opacity='0.9'; this.style.letterSpacing='0.4px'; this.style.transform='scale(1.02) translateY(-1px)'; this.querySelector('.tooltip').style.opacity='1'; this.querySelector('.tooltip').style.transform='translateX(8px)';" onmouseout="this.style.opacity='1'; this.style.letterSpacing='0'; this.style.transform='scale(1) translateY(0)'; this.querySelector('.tooltip').style.opacity='0'; this.querySelector('.tooltip').style.transform='translateX(0)'; Array.from(this.querySelectorAll('.bracket')).forEach(b =&gt; { b.style.transform='scale(1) translateY(0)'; b.style.opacity='0.7'; });" onclick="this.classList.add('copied'); navigator.clipboard.writeText('til040@ucsd.edu'); this.querySelector('.tooltip').textContent='Copied!'; this.style.transform='scale(0.98) translateY(0)'; let circle=document.createElement('span'); circle.className = 'click-effect'; circle.style.cssText = 'position:absolute; pointer-events:none; background:rgba(0,0,0,0.05); border-radius:50%; transform:scale(0); animation:clickEffect 0.5s ease-out; width:100%; height:100%; left:0; top:0;'; this.appendChild(circle); setTimeout(() =&gt; circle.remove(), 500); setTimeout(() =&gt; { this.querySelector('.tooltip').textContent='Click to copy'; this.style.transform='scale(1.02) translateY(-1px)'; }, 1000)"> <style>@keyframes clickEffect{from{transform:scale(0);opacity:.3}to{transform:scale(2);opacity:0}}</style> til040 <span class="bracket" style="opacity: 0.7; transition: transform 0.2s ease-out, opacity 0.2s ease"><span style="opacity: 0.9">ğŸŒ€</span></span> ucsd <span class="bracket" style="opacity: 0.7; transition: transform 0.2s ease-out, opacity 0.2s ease"><span style="opacity: 0.9">âœ¨</span></span> edu <span class="tooltip" style="opacity: 0; position: absolute; left: 100%; transform: translateX(0); font-size: 13px; font-family: 'Courier New', monospace; background: rgba(0,0,0,0.02); padding: 2px 6px; border-radius: 3px; transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1); pointer-events: none; white-space: nowrap; margin-left: 4px; box-shadow: 0 2px 8px rgba(0,0,0,0.02); backdrop-filter: blur(4px);">Click to copy</span> </span> </p> </header> <article> <div class="profile float-right"> <div class="simple-profile-container"> <div class="profile-wrapper rounded"> <img src="/assets/img/me_1.jpg" class="profile-image" alt="Profile image" id="profileImage" loading="eager" decoding="async"> </div> <div class="image-nav-container"> <div class="image-nav"> <button class="nav-btn nav-left" id="navLeft" aria-label="Previous image"> <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" width="20" height="20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"> <polyline points="15 18 9 12 15 6"></polyline> </svg> </button> <div class="image-indicators" id="imageIndicators"> <div class="indicator active" data-index="0"></div> <div class="indicator " data-index="1"></div> <div class="indicator " data-index="2"></div> <div class="indicator " data-index="3"></div> <div class="indicator " data-index="4"></div> <div class="indicator " data-index="5"></div> </div> <button class="nav-btn nav-right" id="navRight" aria-label="Next image"> <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" width="20" height="20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </button> </div> </div> <div style="display: none;" aria-hidden="true"> <img src="/assets/img/me_1.jpg" id="preload-image-0" alt="Preload image 1"> <img src="/assets/img/me_2.jpg" id="preload-image-1" alt="Preload image 2"> <img src="/assets/img/me_3.jpg" id="preload-image-2" alt="Preload image 3"> <img src="/assets/img/me_4.jpg" id="preload-image-3" alt="Preload image 4"> <img src="/assets/img/me_5.jpg" id="preload-image-4" alt="Preload image 5"> <img src="/assets/img/me_6.jpg" id="preload-image-5" alt="Preload image 6"> </div> </div> </div> <div class="clearfix"> <p>Iâ€™m a <strong>Ph.D. student</strong> in Computer Science at <a href="https://ucsd.edu" rel="external nofollow noopener" target="_blank">UC San Diego</a> ğŸ”±, advised by Prof. <a href="https://cseweb.ucsd.edu/~jmcauley/" rel="external nofollow noopener" target="_blank">Julian McAuley</a>. This summer, Iâ€™m an Applied Scientist Intern at AWS AI Labs â˜ï¸, working on the <a href="https://aws.amazon.com/q/developer/" rel="external nofollow noopener" target="_blank">Amazon Q Developer</a> team âš™ï¸ with <a href="https://xiaoyang-wang.github.io/" rel="external nofollow noopener" target="_blank">Xiaoyang Wang</a>, <a href="https://zijianwang.me/" rel="external nofollow noopener" target="_blank">Zijian Wang</a>, and <a href="https://scholar.google.com/citations?user=iw1GQj0AAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Murali Krishna Ramanathan</a>.</p> <p>Previously, I completed my Masterâ€™s degree at UC San Diego ğŸ“, working with Julian McAuley, <a href="http://zhiting.ucsd.edu/index.html" rel="external nofollow noopener" target="_blank">Zhiting Hu</a>, and collaborating with <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> from UC Davis. I also interned at NVIDIA ğŸŸ©, working with <a href="https://www.linkedin.com/in/gaoyan-xie-b2170517/" rel="external nofollow noopener" target="_blank">Gaoyan Xie</a>.</p> <p>I build, train, and evaluate <strong>Large Language Models (LLMs)</strong> ğŸ§  â€” and try to make them smarter ğŸ’¡, stronger ğŸ’ª, and more tasteful ğŸ¨.</p> </div> <h2><a href="/news/" style="color: inherit;">News</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jun 20, 2025</th> <td> ğŸ§™ğŸ» Check out <a href="https://www.arxiv.org/abs/2506.14965" rel="external nofollow noopener" target="_blank">Guru</a>: how cross-domain RL supercharges LLM reasoning. </td> </tr> <tr> <th scope="row">Oct 10, 2024</th> <td> ğŸ¤– We pre-release <a href="https://de-arena.maitrix.org/" rel="external nofollow noopener" target="_blank">Decentralized Arena</a> for automated, scalable, and transparent LLM evaluation. </td> </tr> <tr> <th scope="row">Sep 20, 2024</th> <td> ğŸ‰ <a href="https://arxiv.org/abs/2411.08733" rel="external nofollow noopener" target="_blank">DRPO</a> is accepted to the main conference of EMNLP 2024! </td> </tr> <tr> <th scope="row">Jul 10, 2024</th> <td> ğŸ‰ <a href="https://arxiv.org/abs/2404.05221" rel="external nofollow noopener" target="_blank">LLM Reasoners</a> is accepted to COLM 2024! </td> </tr> <tr> <th scope="row">Feb 28, 2024</th> <td> ğŸ’« We release <a href="https://arxiv.org/abs/2402.19173" rel="external nofollow noopener" target="_blank">StarCoder 2</a>, a family of open LLMs for code. </td> </tr> </table> </div> </div> <h2>Selected Publications <a href="/publications/" style="font-size: 1.4rem;">[view all]</a> </h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div> <div id="cheng2025revisitingreinforcementlearningllm" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2506.14965" target="_blank" rel="external nofollow noopener"><b>Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective</b></a></div> <div class="author"> Zhoujun Cheng*,Â <a href="https://ber666.github.io/" rel="external nofollow noopener" target="_blank">Shibo Hao*</a>,Â <em>Tianyang Liu*</em>,Â Fan Zhou,Â Yutao Xie,Â Feng Yao,Â Yuexin Bian,Â Yonghao Zhuang,Â Nilabjo Dey,Â Yuheng Zha,Â <a href="https://www.yigu.page/" rel="external nofollow noopener" target="_blank">Yi Gu</a>,Â <a href="https://lancelot39.github.io/" rel="external nofollow noopener" target="_blank">Kun Zhou</a>, and <span class="more-authors" title="click to view 12 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '12 more authors' ? 'Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu' : '12 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '4'); ">12 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2506.14965" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://guru-reasoning.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/LLM360/Reasoning360" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://huggingface.co/LLM360/guru-32B" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">model</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domainsâ€“Math, Code, Science, Logic, Simulation, and Tabularâ€“each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cheng2025revisitingreinforcementlearningllm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cheng*, Zhoujun and Hao*, Shibo and Liu*, Tianyang and Zhou, Fan and Xie, Yutao and Yao, Feng and Bian, Yuexin and Zhuang, Yonghao and Dey, Nilabjo and Zha, Yuheng and Gu, Yi and Zhou, Kun and Wang, Yuqi and Li, Yuan and Fan, Richard and She, Jianshu and Gao, Chengqian and Saparov, Abulhair and Li, Haonan and Killian, Taylor W. and Yurochkin, Mikhail and Liu, Zhengzhong and Xing, Eric P. and Hu, Zhiting}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2506.14965}</span><span class="p">,</span>
  <span class="na">dataset</span> <span class="p">=</span> <span class="s">{https://huggingface.co/datasets/LLM360/guru-RL-92k}</span><span class="p">,</span>
  <span class="na">customize</span> <span class="p">=</span> <span class="s">{[model] https://huggingface.co/LLM360/guru-32B}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CVPR</abbr></div> <div id="chen2025symbolic" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2504.17261" target="_blank" rel="external nofollow noopener"><b>Symbolic Representation for Any-to-Any Generative Tasks</b></a></div> <div class="author"> Jiaqi Chen,Â Xiaoye Zhu,Â Yue Wang,Â <em>Tianyang Liu</em>,Â Xinhui Chen,Â Ying Chen,Â Chak Tou Leong,Â Yifei Ke,Â Joseph Liu,Â Yiwen Yuan,Â <a href="https://cseweb.ucsd.edu/~jmcauley/" rel="external nofollow noopener" target="_blank">Julian McAuley</a>,Â andÂ Li-jia Li</div> <div class="periodical"> <em>CVPR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2504.17261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>We propose a symbolic generative task description language and inference engine, capable of representing arbitrary multimodal tasks as symbolic flows. The inference engine maps natural language instructions to symbolic flow, eliminating the need for task-specific training. Conventional generative models rely heavily on large-scale training and implicit neural representation to learn cross-modal mappings, which demands extensive computational resources and restricts expandability. In this paper, we propose an explicit symbolic task descriptive language, comprising three types of primitives: functions, parameters, and topological logic. Using a pre-trained language model to infer symbolic workflows in a training-free manner, our framework successfully performs over 12 multimodal generative tasks based on user instructions, demonstrating enhanced efficiency and flexibility. Extensive experiments demonstrate that our approach can generate multimodal content competitive with, and often surpassing, that of previous state-of-the-art unified models, while offering robust interruptibility and editability. We believe that symbolic task representations are capable of cost-effectively expanding the boundaries of generative AI capabilities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2025symbolic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Symbolic Representation for Any-to-Any Generative Tasks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jiaqi and Zhu, Xiaoye and Wang, Yue and Liu, Tianyang and Chen, Xinhui and Chen, Ying and Leong, Chak Tou and Ke, Yifei and Liu, Joseph and Yuan, Yiwen and McAuley, Julian and Li, Li-jia}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div> <div id="yang2025codethinkthinkcode" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2502.19411" target="_blank" rel="external nofollow noopener"><b>Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs</b></a></div> <div class="author"> Dayu Yang*,Â <em>Tianyang Liu*</em>,Â Daoan Zhang*,Â Antoine Simoulin,Â Xiaoyi Liu,Â Yuwei Cao,Â Zhaopu Teng,Â Xin Qian,Â Grey Yang,Â Jiebo Luo,Â andÂ <a href="https://cseweb.ucsd.edu/~jmcauley/" rel="external nofollow noopener" target="_blank">Julian McAuley</a> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.19411" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLMâ€™s performance in both areas.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yang2025codethinkthinkcode</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang*, Dayu and Liu*, Tianyang and Zhang*, Daoan and Simoulin, Antoine and Liu, Xiaoyi and Cao, Yuwei and Teng, Zhaopu and Qian, Xin and Yang, Grey and Luo, Jiebo and McAuley, Julian}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2502.19411}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI</abbr></div> <div id="chen2025imitate" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2412.10432" target="_blank" rel="external nofollow noopener"><b>Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection</b></a></div> <div class="author"> Jiaqi Chen*,Â Xiaoye Zhu*,Â <em>Tianyang Liu*</em>,Â Ying Chen,Â Xinhui Chen,Â Yiwen Yuan,Â Chak Tou Leong,Â Zuchao Li,Â Long Tang,Â Lei Zhang,Â Chenyu Yan,Â Guanghao Mei, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jie Zhang, Lefei Zhang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '4'); ">2 more authors</span> </div> <div class="periodical"> <em>AAAI</em>, 2025 </div> <div class="periodical"> <strong style="color:#cc3333">Oral Presentation</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.10432" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://machine-text-detection.github.io/ImBD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/Jiaqi-Chen-00/ImBD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://huggingface.co/spaces/machine-text-detection/ImBD" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">demo</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have revolutionized text generation, making detecting machine-generated text increasingly challenging. Although past methods have achieved good performance on detecting pure machine-generated text, those detectors have poor performance on distinguishing machine-revised text (rewriting, expansion, and polishing), which can have only minor changes from its original human prompt. As the content of text may originate from human prompts, detecting machine-revised text often involves identifying distinctive machine styles, e.g., worded favored by LLMs. However, existing methods struggle to detect machine-style phrasing hidden within the content contributed by humans. We propose the "Imitate Before Detect" (ImBD) approach, which first imitates the machine-style token distribution, and then compares the distribution of the text to be tested with the machine-style distribution to determine whether the text has been machine-revised. To this end, we introduce style preference optimization (SPO), which aligns a scoring LLM model to the preference of text styles generated by machines. The aligned scoring model is then used to calculate the style-conditional probability curvature (Style-CPC), quantifying the log probability difference between the original and conditionally sampled texts for effective detection. We conduct extensive comparisons across various scenarios, encompassing text revisions by six LLMs, four distinct text domains, and three machine revision types. Compared to existing state-of-the-art methods, our method yields a 13% increase in AUC for detecting text revised by open-source LLMs, and improves performance by 5% and 19% for detecting GPT-3.5 and GPT-4o revised text, respectively. Notably, our method surpasses the commercially trained GPT-Zero with just 1,000 samples and five minutes of SPO, demonstrating its efficiency and effectiveness.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chen2025imitate</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen*, Jiaqi and Zhu*, Xiaoye and Liu*, Tianyang and Chen, Ying and Chen, Xinhui and Yuan, Yiwen and Leong, Chak Tou and Li, Zuchao and Tang, Long and Zhang, Lei and Yan, Chenyu and Mei, Guanghao and Zhang, Jie and Zhang, Lefei}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{AAAI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">customize</span> <span class="p">=</span> <span class="s">{[demo] https://huggingface.co/spaces/machine-text-detection/ImBD}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2412.10432}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{&lt;strong style="color:#cc3333"&gt;Oral Presentation&lt;/strong&gt;}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div> <div id="yin2025decentralized" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2505.12808" target="_blank" rel="external nofollow noopener"><b>Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models</b></a></div> <div class="author"> Yanbin Yin,Â <a href="https://lancelot39.github.io/" rel="external nofollow noopener" target="_blank">Kun Zhou</a>,Â <a href="https://zhenwang9102.github.io/" rel="external nofollow noopener" target="_blank">Zhen Wang</a>,Â Xiangdong Zhang,Â Yifei Shao,Â <a href="https://ber666.github.io/" rel="external nofollow noopener" target="_blank">Shibo Hao</a>,Â <a href="https://www.yigu.page/" rel="external nofollow noopener" target="_blank">Yi Gu</a>,Â Jieyuan Liu,Â Somanshu Singla,Â <em>Tianyang Liu</em>,Â <a href="https://www.cs.cmu.edu/~epxing/" rel="external nofollow noopener" target="_blank">Eric P. Xing</a>,Â Zhengzhong Liu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Haojian Jin, Zhiting Hu' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '4'); ">2 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.12808" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://de-arena.maitrix.org/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on this https URL.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">yin2025decentralized</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yin, Yanbin and Zhou, Kun and Wang, Zhen and Zhang, Xiangdong and Shao, Yifei and Hao, Shibo and Gu, Yi and Liu, Jieyuan and Singla, Somanshu and Liu, Tianyang and Xing, Eric P. and Liu, Zhengzhong and Jin, Haojian and Hu, Zhiting}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2505.12808}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP (main)</abbr></div> <div id="singla2024dynamic" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2411.08733" target="_blank" rel="external nofollow noopener"><b>Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</b></a></div> <div class="author"> Somanshu Singla*,Â <a href="https://zhenwang9102.github.io/" rel="external nofollow noopener" target="_blank">Zhen Wang*</a>,Â <em>Tianyang Liu</em>,Â Abdullah Ashfaq,Â <a href="http://zhiting.ucsd.edu/index.html" rel="external nofollow noopener" target="_blank">Zhiting Hu</a>,Â andÂ <a href="https://www.cs.cmu.edu/~epxing/" rel="external nofollow noopener" target="_blank">Eric P. Xing</a> </div> <div class="periodical"> <em>EMNLP</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2411.08733" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Singla17/dynamic-alignment-optimization" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Aligning Large Language Models (LLMs) traditionally relies on costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To enable alignment without these expensive tuning and annotation, we present a new tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPOâ€™s automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">singla2024dynamic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Singla*, Somanshu and Wang*, Zhen and Liu, Tianyang and Ashfaq, Abdullah and Hu, Zhiting and Xing, Eric P.}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{EMNLP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">COLM</abbr></div> <div id="hao2024llm" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2404.05221" target="_blank" rel="external nofollow noopener"><b>LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models</b></a></div> <div class="author"> <a href="https://ber666.github.io/" rel="external nofollow noopener" target="_blank">Shibo Hao</a>,Â <a href="https://www.yigu.page/" rel="external nofollow noopener" target="_blank">Yi Gu</a>,Â Haotian Luo,Â <em>Tianyang Liu</em>,Â Xiyan Shao,Â Xinyuan Wang,Â Shuhua Xie,Â Haodi Ma,Â Adithya Samavedhi,Â Qiyue Gao,Â <a href="https://zhenwang9102.github.io/" rel="external nofollow noopener" target="_blank">Zhen Wang</a>,Â andÂ <a href="http://zhiting.ucsd.edu/index.html" rel="external nofollow noopener" target="_blank">Zhiting Hu</a> </div> <div class="periodical"> <em>COLM</em>, 2024 </div> <div class="periodical"> Also to appear at Large Language Model (LLM) Agents workshop at ICLR 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.05221" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/maitrix-org/llm-reasoners" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hao2024llm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hao, Shibo and Gu, Yi and Luo, Haotian and Liu, Tianyang and Shao, Xiyan and Wang, Xinyuan and Xie, Shuhua and Ma, Haodi and Samavedhi, Adithya and Gao, Qiyue and Wang, Zhen and Hu, Zhiting}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{COLM}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Language Modeling}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Also to appear at Large Language Model (LLM) Agents workshop at ICLR 2024}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div> <div id="starcoder2" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2402.19173" target="_blank" rel="external nofollow noopener"><b>StarCoder 2 and The Stack v2: The Next Generation</b></a></div> <div class="author"> Anton Lozhkov,Â Raymond Li,Â Loubna Ben Allal,Â Federico Cassano,Â Joel Lamy-Poirier,Â Nouamane Tazi,Â Ao Tang,Â Dmytro Pykhtar,Â Jiawei Liu,Â Yuxiang Wei,Â <em>Tianyang Liu</em>,Â Max Tian, and <span class="more-authors" title="click to view 54 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '54 more authors' ? 'Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas KrauÃŸ, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos MuÃ±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Werra, Harm Vries' : '54 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '4'); ">54 more authors</span> </div> <div class="periodical"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.19173" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/blog/starcoder2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/bigcode-project/starcoder2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">starcoder2</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StarCoder 2 and The Stack v2: The Next Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and Liu, Tianyang and Tian, Max and Kocetkov, Denis and Zucker, Arthur and Belkada, Younes and Wang, Zijian and Liu, Qian and Abulkhanov, Dmitry and Paul, Indraneil and Li, Zhuang and Li, Wen-Ding and Risdal, Megan and Li, Jia and Zhu, Jian and Zhuo, Terry Yue and Zheltonozhskii, Evgenii and Dade, Nii Osae Osae and Yu, Wenhao and KrauÃŸ, Lucas and Jain, Naman and Su, Yixuan and He, Xuanli and Dey, Manan and Abati, Edoardo and Chai, Yekun and Muennighoff, Niklas and Tang, Xiangru and Oblokulov, Muhtasham and Akiki, Christopher and Marone, Marc and Mou, Chenghao and Mishra, Mayank and Gu, Alex and Hui, Binyuan and Dao, Tri and Zebaze, Armel and Dehaene, Olivier and Patry, Nicolas and Xu, Canwen and McAuley, Julian and Hu, Han and Scholak, Torsten and Paquet, Sebastien and Robinson, Jennifer and Anderson, Carolyn Jane and Chapados, Nicolas and Patwary, Mostofa and Tajbakhsh, Nima and Jernite, Yacine and Ferrandis, Carlos MuÃ±oz and Zhang, Lingming and Hughes, Sean and Wolf, Thomas and Guha, Arjun and von Werra, Leandro and de Vries, Harm}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div> <div id="liu2023rethinking" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2312.16702" target="_blank" rel="external nofollow noopener"><b>Rethinking Tabular Data Understanding of Large Language Models</b></a></div> <div class="author"> <em>Tianyang Liu</em>,Â <a href="https://feiwang96.github.io/" rel="external nofollow noopener" target="_blank">Fei Wang</a>,Â andÂ <a href="https://muhaochen.github.io/" rel="external nofollow noopener" target="_blank">Muhao Chen</a> </div> <div class="periodical"> <em>NAACL</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2312.16702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Leolty/tablellm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2023rethinking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Rethinking Tabular Data Understanding of Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Tianyang and Wang, Fei and Chen, Muhao}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NAACL}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Conference of the North American Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="liu2023repobench" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2306.03091" target="_blank" rel="external nofollow noopener"><b>RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems</b></a></div> <div class="author"> <em>Tianyang Liu</em>,Â <a href="https://www.canwenxu.net/" rel="external nofollow noopener" target="_blank">Canwen Xu</a>,Â andÂ <a href="https://cseweb.ucsd.edu/~jmcauley/" rel="external nofollow noopener" target="_blank">Julian McAuley</a> </div> <div class="periodical"> <em>ICLR</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2306.03091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/leolty/RepoBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the systemâ€™s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/leolty/RepoBench</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2023repobench</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Tianyang and Xu, Canwen and McAuley, Julian}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="hao2023toolkengpt" class="col-sm-8"> <div class="title"><a href="http://arxiv.org/abs/2305.11554" target="_blank" rel="external nofollow noopener"><b>ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings</b></a></div> <div class="author"> <a href="https://ber666.github.io/" rel="external nofollow noopener" target="_blank">Shibo Hao</a>,Â <em>Tianyang Liu</em>,Â <a href="https://zhenwang9102.github.io/" rel="external nofollow noopener" target="_blank">Zhen Wang</a>,Â andÂ <a href="http://zhiting.ucsd.edu/index.html" rel="external nofollow noopener" target="_blank">Zhiting Hu</a> </div> <div class="periodical"> <em>NeurIPS</em>, 2023 </div> <div class="periodical"> <strong style="color:#cc3333">Oral (67 out of 12345 submissions), Best Paper Award at SoCal NLP 2023</strong> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.11554" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Ber666/ToolkenGPT" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/toolkenGPT-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="badges"> <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, <strong>ToolkenGPT</strong>, which combines the benefits of both sides. Our approach represents each <u>tool</u> as a <u>ken</u> (i.e., toolken) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hao2023toolkengpt</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{NeurIPS}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{&lt;strong style="color:#cc3333"&gt;Oral (67 out of 12345 submissions), Best Paper Award at SoCal NLP 2023&lt;/strong&gt;}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <h2>Services</h2> <div class="services-section"> <div class="services-content"> <h3 class="services-role">Invited Reviewer</h3> <div class="services-timeline"> <div class="timeline-item"> <div class="year">2025</div> <ul> <li>AAAI</li> <li>AISTATS</li> <li>ACL ARR (Feb)</li> <li>COLM</li> <li>ICLR</li> <li>ICML</li> </ul> </div> <div class="timeline-item"> <div class="year">2024</div> <ul> <li>ACL ARR (Feb, Apr, Jun, Aug, Oct, Dec)</li> <li>COLM</li> <li>ICLR</li> <li>ICML</li> <li>NeurIPS</li> </ul> </div> <div class="timeline-item"> <div class="year">2023</div> <ul> <li>ACL ARR (Dec)</li> <li>NLPCC</li> </ul> </div> </div> </div> </div> </article> </div> <style>.profile-note{font-size:.62rem;margin-top:0}.simple-profile-container{position:relative;margin-bottom:10px}.profile-wrapper{position:relative;width:100%;overflow:hidden;box-shadow:0 3px 12px rgba(0,0,0,0.12);margin-bottom:18px;aspect-ratio:1/1}.profile-wrapper.circular{border-radius:50%}.profile-wrapper.rounded{border-radius:12px}.profile-image{width:100%;height:100%;display:block;object-fit:cover;object-position:center;transition:opacity .6s ease}.image-nav-container{margin-top:12px}.image-nav{display:flex;align-items:center;justify-content:center;gap:16px}.nav-btn{background:rgba(0,0,0,0.1);border:0;border-radius:50%;width:36px;height:36px;display:flex;align-items:center;justify-content:center;cursor:pointer;transition:all .2s ease;color:rgba(0,0,0,0.6)}.nav-btn:hover{background:rgba(0,0,0,0.15);color:rgba(0,0,0,0.8);transform:scale(1.05)}.nav-btn:active{transform:scale(0.95)}.image-indicators{display:flex;gap:8px;align-items:center}.indicator{width:8px;height:8px;border-radius:50%;background:rgba(0,0,0,0.2);cursor:pointer;transition:all .3s ease}.indicator.active{background:rgba(0,0,0,0.6);transform:scale(1.2)}.indicator:hover{background:rgba(0,0,0,0.4);transform:scale(1.1)}html[data-theme="dark"] .nav-btn{background:rgba(255,255,255,0.1);color:rgba(255,255,255,0.6)}html[data-theme="dark"] .nav-btn:hover{background:rgba(255,255,255,0.15);color:rgba(255,255,255,0.8)}html[data-theme="dark"] .indicator{background:rgba(255,255,255,0.2)}html[data-theme="dark"] .indicator.active{background:rgba(255,255,255,0.6)}html[data-theme="dark"] .indicator:hover{background:rgba(255,255,255,0.4)}</style> <script>function switchToImage(e){if(totalImages<=1)return;let t;if("next"===e)t=(currentImageIndex+1)%totalImages;else if("prev"===e)t=(currentImageIndex-1+totalImages)%totalImages;else{if("number"!=typeof e)return;t=e}if(t===currentImageIndex)return;console.log("Switching from image",currentImageIndex,"to",t),console.log("New image path:",imagePaths[t]);const o=document.getElementById("profileImage");o&&imagePaths[t]?(o.style.opacity="0",setTimeout(()=>{o.src=imagePaths[t],o.style.opacity="1",console.log("Image switched successfully")},300)):console.error("Failed to switch image:",o?"Image element found":"No image element",imagePaths[t]?"Path exists":"No image path"),updateIndicators(t),currentImageIndex=t,restartAutoSwitch()}function updateIndicators(e){document.querySelectorAll(".indicator").forEach((t,o)=>{o===e?t.classList.add("active"):t.classList.remove("active")})}function startAutoSwitch(){if(totalImages<=1)return;let e;stopAutoSwitch(),e=0===currentImageIndex?15e3:5e3,console.log("Starting auto switch timer for",e,"ms (current index:",currentImageIndex,")"),autoSwitchInterval=setTimeout(()=>{console.log("Auto switch triggered"),switchToImage("next")},e)}function stopAutoSwitch(){autoSwitchInterval&&(clearInterval(autoSwitchInterval),autoSwitchInterval=null)}function restartAutoSwitch(){stopAutoSwitch(),startAutoSwitch()}let autoSwitchInterval,currentImageIndex=0,totalImages=0,imagePaths=[];document.addEventListener("DOMContentLoaded",function(){document.getElementById("profileImage");const e=document.getElementById("imageIndicators"),t=document.getElementById("navLeft"),o=document.getElementById("navRight"),n=document.querySelectorAll('[id^="preload-image-"]');if(totalImages=n.length,console.log("Found",totalImages,"images for switching"),totalImages<=1)return void console.log("Only one image found, no switching needed");if(n.forEach((e,t)=>{imagePaths[t]=e.src,console.log("Image",t+":",e.src)}),t&&o&&(t.addEventListener("click",()=>switchToImage("prev")),o.addEventListener("click",()=>switchToImage("next")),console.log("Navigation buttons set up")),e){e.querySelectorAll(".indicator").forEach((e,t)=>{e.addEventListener("click",()=>switchToImage(t))}),console.log("Indicator dots set up")}startAutoSwitch(),console.log("Auto switching started");const a=document.querySelector(".simple-profile-container");a&&(a.addEventListener("mouseenter",stopAutoSwitch),a.addEventListener("mouseleave",startAutoSwitch))});</script> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2025 Tianyang Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: July 17, 2025. </div> <div id="map-container" style="display: none;"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=300&amp;t=n&amp;d=vcetvMKxQeU0A74GGVddvtKdDYpzY562Hjs3OBdOjBw"></script> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/live2d-widget@3.0.4/lib/L2Dwidget.min.js"></script> <style>#live2dcanvas{border:none!important}@media screen and (max-width:768px){#live2dcanvas{transform:scale(0.47)!important;transform-origin:bottom right!important}}</style> <script>var config={model:{jsonPath:"https://cdn.jsdelivr.net/gh/cc963020001/myproject@1.0/kmusume/2d-mao/hijiki.model.json"},display:{superSample:1.5,width:160,height:160,position:"right",hOffset:0,vOffset:0},mobile:{show:!0,scale:.25,motion:!0},react:{opacityDefault:1,opacityOnHover:.75}};L2Dwidget.init(config);</script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>