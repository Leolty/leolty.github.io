---

@article{cocoabench2025,
  abbr={preprint},
  bibtex_show={true},
  title={CocoaBench: An Evaluation Framework for General Agents with Compositional Cognitive Abilities},
  author={Shibo Hao* and Zhining Zhang* and Zhiqi Liang* and Tianyang Liu* and Zilong Wang* and Kun Zhou and Yuheng Zha and Qiyue Gao and Jixuan Chen and Zhoujun Cheng and Yu Wang and Feng Yao and Licheng Liu and Ziqiao Ma and Hector Liu and Rupesh Srivastava and Julian McAuley and Jingbo Shang and Lianhui Qin and Zhiting Hu},
  journal={arXiv preprint},
  year={2025},
  blog={https://cocoabench.github.io/},
  code={https://github.com/cocoabench/cocoa-agent},
  customize={[leaderboard] https://cocoabench.github.io/leaderboard.html},
  abstract={We introduce CocoaBench, an evaluation framework for general agents with compositional cognitive abilities. We focus on evaluating general agents on highly complex tasks, while only requiring them to operate a set of general tools (browser, code execution, file systems, etc.). We design the benchmark around cognitive abilities rather than specific tools. The interfaces through which LLMs interact with the world will change, but the underlying abilities required—perception, reasoning, and memory—remain unchanged. The benchmark is easy to use: tasks are human-understandable and automatically evaluated by scripts. We additionally developed a framework that makes it straightforward to evaluate any models within a lightweight sandbox. Our current benchmark (CocoaBench-0.1) includes 25 human-curated tasks. We evaluate several leading commercial agent systems and find that none are able to correctly complete more than 50% of the tasks. Among all tested agents, the OpenAI Agent demonstrates the strongest capabilities in solving complex tasks with diverse cognitive abilities. To enable rigorous evaluation and empower researchers to develop their own agents, we built the CocoaAgent framework, which provides seamless integration with AIO Sandbox and equips agents with a full suite of tools.},
  selected={true}
}

@article{bigcodearena2025,
  abbr={preprint},
  bibtex_show={true},
  title={BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution},
  author={Terry Yue Zhuo and Xiaolong Jin and Hange Liu and Juyong Jiang and Tianyang Liu and Chen Gong and Bhupesh Bishnoi and Vaisakhi Mishra and Marek Suppa and Noah Ziems and Saiteja Utpala and Ming Xu and Guangyu Song and Kaixin Li and Yuhan Cao and Bo Liu and Zheng Liu and Sabina Abdurakhmanova and Wenhao Yu and Mengzhao Jia and Jihan Yao and Kenneth Hamilton and Kumar Shridhar and Minh Chien Vu and Dingmin Wang and Jiawei Liu and Zijian Wang and Qian Liu and Binyuan Hui and Meg Risdal and Ahsen Khaliq and Atin Sood and Zhenchang Xing and Wasi Uddin Ahmad and John Grundy and David Lo and Banghua Zhu and Xiaoning Du and Torsten Scholak and Leandro von Werra},
  journal={arXiv preprint},
  year={2025},
  arxiv={2510.08697},
  url={https://arxiv.org/abs/2510.08697},
  code={https://github.com/bigcode-project/bigcodearena},
  blog={https://huggingface.co/blog/bigcode/arena},
  customize={[website] https://huggingface.co/spaces/bigcode/arena},
  abstract={Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.},
  selected={true}
}


@article{bian2025dontknowclickautomatedgui,
  abbr={preprint},
  bibtex_show={true},
  title={You Don't Know Until You Click: Automated GUI Testing for Production-Ready Software Evaluation},
  author={Yutong Bian and Xianhao Lin and Yupeng Xie and Tianyang Liu and Mingchen Zhuge and Siyuan Lu and Haoming Tang and Jinlin Wang and Jiayi Zhang and Jiaqi Chen and Xiangru Tang and Yongxin Ni and Sirui Hong and Chenglin Wu},
  journal={arXiv preprint},
  year={2025},
  arxiv={2508.14104},
  url={https://arxiv.org/abs/2508.14104},
  abstract={Large Language Models (LLMs) and code agents in software development are rapidly evolving from generating isolated code snippets to producing full-fledged software applications with graphical interfaces, interactive logic, and dynamic behaviors. However, current benchmarks fall short in evaluating such production-ready software, as they often rely on static checks or binary pass/fail scripts, failing to capture the interactive behaviors and runtime dynamics that define real-world usability - qualities that only emerge when an application is actively used. This is the blind spot of current evaluation: you don't know if an app works until you click through it, interact with it, and observe how it responds. To bridge this gap, we introduce RealDevWorld, a novel evaluation framework for automated end-to-end assessment of LLMs' ability to generate production-ready repositories from scratch. It features two key components: (1) RealDevBench, a diverse collection of 194 open-ended software engineering tasks across multiple domains, incorporating multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a new agent-as-a-judge evaluation system that simulates realistic, GUI-based user interactions to automatically and holistically assess software functional correctness, visual fidelity, and runtime behavior. The framework delivers fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation beyond simple success/failure judgments. Empirical results show that RealDevWorld delivers effective, automatic, and human-aligned evaluations, achieving an accuracy of 0.92 and a correlation of 0.85 with expert human assessments, while significantly reducing the reliance on manual review. This enables scalable, human-aligned assessment of production-level software generated by LLMs. Our code is available on GitHub.},
  selected={true},
  code={https://github.com/tanghaom/AppEvalPilot}
}

@article{cheng2025revisitingreinforcementlearningllm,
  abbr={NeurIPS},
  bibtex_show={true},
  title={Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective}, 
  author={Zhoujun Cheng* and Shibo Hao* and Tianyang Liu* and Fan Zhou and Yutao Xie and Feng Yao and Yuexin Bian and Yonghao Zhuang and Nilabjo Dey and Yuheng Zha and Yi Gu and Kun Zhou and Yuqi Wang and Yuan Li and Richard Fan and Jianshu She and Chengqian Gao and Abulhair Saparov and Haonan Li and Taylor W. Killian and Mikhail Yurochkin and Zhengzhong Liu and Eric P. Xing and Zhiting Hu},
  journal={NeurIPS},
  year={2025},
  arxiv={2506.14965},
  url={https://arxiv.org/abs/2506.14965},
  blog={https://guru-reasoning.github.io/},
  code={https://github.com/LLM360/Reasoning360},
  dataset={https://huggingface.co/datasets/LLM360/guru-RL-92k},
  customize={[model] https://huggingface.co/LLM360/guru-32B},
  abstract={Reinforcement learning (RL) has emerged as a promising approach to improve large language model (LLM) reasoning, yet most open efforts focus narrowly on math and code, limiting our understanding of its broader applicability to general reasoning. A key challenge lies in the lack of reliable, scalable RL reward signals across diverse reasoning domains. We introduce Guru, a curated RL reasoning corpus of 92K verifiable examples spanning six reasoning domains--Math, Code, Science, Logic, Simulation, and Tabular--each built through domain-specific reward design, deduplication, and filtering to ensure reliability and effectiveness for RL training. Based on Guru, we systematically revisit established findings in RL for LLM reasoning and observe significant variation across domains. For example, while prior work suggests that RL primarily elicits existing knowledge from pretrained models, our results reveal a more nuanced pattern: domains frequently seen during pretraining (Math, Code, Science) easily benefit from cross-domain RL training, while domains with limited pretraining exposure (Logic, Simulation, and Tabular) require in-domain training to achieve meaningful performance gains, suggesting that RL is likely to facilitate genuine skill acquisition. Finally, we present Guru-7B and Guru-32B, two models that achieve state-of-the-art performance among open models RL-trained with publicly available data, outperforming best baselines by 7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We also show that our models effectively improve the Pass@k performance of their base models, particularly on complex tasks less likely to appear in pretraining data. We release data, models, training and evaluation code to facilitate general-purpose reasoning at: https://github.com/LLM360/Reasoning360.},
  selected={true}
}

@article{chen2025symbolic,
  abbr={CVPR},
  bibtex_show={true},
  title={Symbolic Representation for Any-to-Any Generative Tasks},
  author={Jiaqi Chen and Xiaoye Zhu and Yue Wang and Tianyang Liu and Xinhui Chen and Ying Chen and Chak Tou Leong and Yifei Ke and Joseph Liu and Yiwen Yuan and Julian McAuley and Li-jia Li},
  journal={CVPR},
  year={2025},
  arxiv={2504.17261},
  selected={true},
  abstract={We propose a symbolic generative task description language and inference engine, capable of representing arbitrary multimodal tasks as symbolic flows. The inference engine maps natural language instructions to symbolic flow, eliminating the need for task-specific training. Conventional generative models rely heavily on large-scale training and implicit neural representation to learn cross-modal mappings, which demands extensive computational resources and restricts expandability. In this paper, we propose an explicit symbolic task descriptive language, comprising three types of primitives: functions, parameters, and topological logic. Using a pre-trained language model to infer symbolic workflows in a training-free manner, our framework successfully performs over 12 multimodal generative tasks based on user instructions, demonstrating enhanced efficiency and flexibility. Extensive experiments demonstrate that our approach can generate multimodal content competitive with, and often surpassing, that of previous state-of-the-art unified models, while offering robust interruptibility and editability. We believe that symbolic task representations are capable of cost-effectively expanding the boundaries of generative AI capabilities.},
}

@article{yang2025codethinkthinkcode,
  abbr={EMNLP (main)},
  bibtex_show={true},
  title={Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs}, 
  author={Dayu Yang* and Tianyang Liu* and Daoan Zhang* and Antoine Simoulin and Xiaoyi Liu and Yuwei Cao and Zhaopu Teng and Xin Qian and Grey Yang and Jiebo Luo and Julian McAuley},
  journal={EMNLP},
  year={2025},
  arxiv={2502.19411},
  url={https://arxiv.org/abs/2502.19411},
  abstract={In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.},
  selected={true}
}


@article{chen2025imitate,
  abbr={AAAI},
  bibtex_show={true},
  title={Imitate Before Detect: Aligning Machine Stylistic Preference for Machine-Revised Text Detection},
  author={Jiaqi Chen* and Xiaoye Zhu* and Tianyang Liu* and Ying Chen and Xinhui Chen and Yiwen Yuan and Chak Tou Leong and Zuchao Li and Long Tang and Lei Zhang and Chenyu Yan and Guanghao Mei and Jie Zhang and Lefei Zhang},
  journal={AAAI},
  year={2025},
  arxiv={2412.10432},
  code={https://github.com/Jiaqi-Chen-00/ImBD},
  customize={[demo] https://huggingface.co/spaces/machine-text-detection/ImBD},
  blog={https://machine-text-detection.github.io/ImBD},
  url={https://arxiv.org/abs/2412.10432},
  note={<strong style="color:#cc3333">Oral Presentation</strong>},
  selected={true},
  abstract={Large Language Models (LLMs) have revolutionized text generation, making detecting machine-generated text increasingly challenging. Although past methods have achieved good performance on detecting pure machine-generated text, those detectors have poor performance on distinguishing machine-revised text (rewriting, expansion, and polishing), which can have only minor changes from its original human prompt. As the content of text may originate from human prompts, detecting machine-revised text often involves identifying distinctive machine styles, e.g., worded favored by LLMs. However, existing methods struggle to detect machine-style phrasing hidden within the content contributed by humans. We propose the "Imitate Before Detect" (ImBD) approach, which first imitates the machine-style token distribution, and then compares the distribution of the text to be tested with the machine-style distribution to determine whether the text has been machine-revised. To this end, we introduce style preference optimization (SPO), which aligns a scoring LLM model to the preference of text styles generated by machines. The aligned scoring model is then used to calculate the style-conditional probability curvature (Style-CPC), quantifying the log probability difference between the original and conditionally sampled texts for effective detection. We conduct extensive comparisons across various scenarios, encompassing text revisions by six LLMs, four distinct text domains, and three machine revision types. Compared to existing state-of-the-art methods, our method yields a 13% increase in AUC for detecting text revised by open-source LLMs, and improves performance by 5% and 19% for detecting GPT-3.5 and GPT-4o revised text, respectively. Notably, our method surpasses the commercially trained GPT-Zero with just 1,000 samples and five minutes of SPO, demonstrating its efficiency and effectiveness.},
}

@article{yin2025decentralized,
  abbr={arXiv},
  bibtex_show={true},
  title={Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models},
  author={Yanbin Yin and Kun Zhou and Zhen Wang and Xiangdong Zhang and Yifei Shao and Shibo Hao and Yi Gu and Jieyuan Liu and Somanshu Singla and Tianyang Liu and Eric P. Xing and Zhengzhong Liu and Haojian Jin and Zhiting Hu},
  year={2025},
  journal={arXiv preprint},
  arxiv={2505.12808},
  url={https://arxiv.org/abs/2505.12808},
  blog={https://de-arena.maitrix.org/},
  abstract={The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on this https URL.},
  selected = {true},
}


@article{singla2024dynamic,
  abbr={EMNLP (main)},
  bibtex_show={true},
  title={Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models},
  author={Somanshu Singla* and Zhen Wang* and Tianyang Liu and Abdullah Ashfaq and Zhiting Hu and Eric P. Xing},
  journal={EMNLP},
  year={2024},
  abstract={Aligning Large Language Models (LLMs) traditionally relies on  costly training processes like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). To enable alignment without these expensive tuning and annotation, we present a new tuning-free approach for self-alignment called Dynamic Rewarding with Prompt Optimization (DRPO). Our approach enables self-alignment through a search-based prompt optimization framework, allowing the model to self-improve and generate optimized prompts without additional training or human supervision. The core of DRPO leverages a dynamic rewarding mechanism to identify and rectify model-specific alignment weaknesses, enabling LLMs to adapt quickly to various alignment challenges. Empirical evaluations on eight recent LLMs, including both open- and closed-source, reveal that DRPO significantly enhances alignment performance, enabling base models to outperform their SFT/RLHF-tuned counterparts. Moreover, DRPO's automatically optimized prompts surpass those curated by human experts, demonstrating its superior alignment capabilities. Our findings envision a highly cost-effective and adaptable solution for future alignment research to be further explored.},
  arxiv={2411.08733},
  code={https://github.com/Singla17/dynamic-alignment-optimization},
  selected={true}
}

@article{hao2024llm,
  abbr={COLM},
  bibtex_show={true},
  title={LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models}, 
  author={Shibo Hao and Yi Gu and Haotian Luo and Tianyang Liu and Xiyan Shao and Xinyuan Wang and Shuhua Xie and Haodi Ma and Adithya Samavedhi and Qiyue Gao and Zhen Wang and Zhiting Hu},
  arxiv={2404.05221},
  journal={COLM},
  code={https://github.com/maitrix-org/llm-reasoners},
  booktitle={Conference on Language Modeling},
  note={Also to appear at Large Language Model (LLM) Agents workshop at ICLR 2024},
  year={2024},
  abstract={Generating accurate step-by-step reasoning is essential for Large Language Models (LLMs) to address complex problems and enhance robustness and interpretability. Despite the flux of research on developing advanced reasoning approaches, systematically analyzing the diverse LLMs and reasoning strategies in generating reasoning chains remains a significant challenge. The difficulties stem from the lack of two key elements: (1) an automatic method for evaluating the generated reasoning chains on different tasks, and (2) a unified formalism and implementation of the diverse reasoning approaches for systematic comparison. This paper aims to close the gap: (1) We introduce AutoRace for fully automated reasoning chain evaluation. Existing metrics rely on expensive human annotations or pre-defined LLM prompts not adaptable to different tasks. In contrast, AutoRace automatically creates detailed evaluation criteria tailored for each task, and uses GPT-4 for accurate evaluation following the criteria. (2) We develop LLM Reasoners, a library for standardized modular implementation of existing and new reasoning algorithms, under a unified formulation of the search, reward, and world model components. With the new evaluation and library, (3) we conduct extensive study of different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals interesting findings about different factors contributing to reasoning, including the reward-guidance, breadth-vs-depth in search, world model, and prompt formats, etc.},
  selected={true}
}

@article{starcoder2,
  abbr={technical report},
  bibtex_show={true},
  title={StarCoder 2 and The Stack v2: The Next Generation}, 
  author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
  arxiv={2402.19173},
  journal={arXiv preprint},
  code={https://github.com/bigcode-project/starcoder2},
  blog={https://huggingface.co/blog/starcoder2},
  year={2024},
  abstract={The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.},
  selected={true}
}


@article{liu2023rethinking,
  abbr={NAACL},
  bibtex_show={true},
  title={Rethinking Tabular Data Understanding of Large Language Models},
  abstract = {Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.},
  author={Tianyang Liu and Fei Wang and Muhao Chen},
  arxiv={2312.16702},
  journal={NAACL},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2024},
  code={https://github.com/Leolty/tablellm},
  selected={true}
}

@article{liu2023repobench,
  abbr={ICLR},
  bibtex_show={true},
  author = {Tianyang Liu and Canwen Xu and Julian McAuley},
  title = {RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems},
  abstract = {Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/leolty/RepoBench},
  arxiv = {2306.03091},
  journal={ICLR},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  code = {https://github.com/leolty/RepoBench},
  selected = {true}
}

@article{hao2023toolkengpt,
  abbr={NeurIPS},
  bibtex_show={true},
  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings}, 
  abstract = {Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, <strong>ToolkenGPT</strong>, which combines the benefits of both sides. Our approach represents each <u>tool</u> as a <u>ken</u> (i.e., toolken) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.},
  author={Shibo Hao and Tianyang Liu and Zhen Wang and Zhiting Hu},
  journal={NeurIPS},
  note={<strong style="color:#cc3333">Oral (67 out of 12345 submissions), Best Paper Award at SoCal NLP 2023</strong>},
  year={2023},
  arxiv={2305.11554},
  selected={true},
  code={https://github.com/Ber666/ToolkenGPT},
  poster={toolkenGPT-poster.pdf}
}


@article{liu2023rosematcher,
  abbr={IST},
  bibtex_show={true},
  title = {RoseMatcher: Identifying the Impact of User Reviews on App Updates},
  journal = {Information and Software Technology},
  volume = {161},
  pages = {107261},
  year = {2023},
  url = {https://www.sciencedirect.com/science/article/pii/S0950584923001155},
  author = {Tianyang Liu and Chong Wang and Kun Huang and Peng Liang and Beiqi Zhang and Maya Daneva and Marten {van Sinderen}},
  abstract = {<strong>Context</strong>:
  The release planning of mobile apps has become an area of active research, with most studies centering on app analysis through release notes in the Apple App Store and tracking user reviews via issue trackers. However, the correlation between these release notes and user reviews in App Store remains understudied.
  <strong>Objective</strong>:
  In this paper, we introduce RoseMatcher, a novel automatic approach to match relevant user reviews with app release notes, and identify matched pairs with high confidence.
  <strong>Methods</strong>:
  We collected 944 release notes and 1,046,862 user reviews from 5 mobile apps in the Apple App Store as research data to evaluate the effectiveness and accuracy of RoseMatcher, and conducted deep content analysis on matched pairs.
  <strong>Results</strong>:
  Our evaluation shows that RoseMatcher can reach a hit ratio of 0.718 for identifying relevant matched pairs, and with the manual labeling and content analysis of 984 relevant pairs, we identify 8 roles that user reviews play in app updates according to the relationship between release notes and user reviews in the relevant matched pairs.
  <strong>Conclusions</strong>:
  Our findings indicate that both app development teams and users pay close attention to release notes and user reviews, with release notes typically addressing feature requests, bug reports, and complaints, and user reviews offering positive, negative, and constructive feedback. Overall, the study highlights the importance of the communication between app development teams and users in the release planning of mobile apps, with relevant reviews tending to be posed within a short period before and after the release of release notes, with the average time interval between the post time of release notes and user reviews being approximately one year.}
}

@inproceedings{zhang2023architecture,
  abbr={SANER},
  bibtex_show={true},
  author={Zhang, Beiqi and Liu, Tianyang and Liang, Peng and Wang, Chong and Shahin, Mojtaba and Yu, Jiaxin},
  booktitle={Proceedings of SANER}, 
  title={Architecture Decisions in AI-based Systems Development: An Empirical Study}, 
  year={2023},
  pages={616-626},
  abstract={Artificial Intelligence (AI) technologies have been developed rapidly, and AI-based systems have been widely used in various application domains with opportunities and challenges. However, little is known about the architecture decisions made in AI-based systems development, which has a substantial impact on the success and sustainability of these systems. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub. More specifically, we searched on SO with six sets of keywords and explored 32 AI-based projects on GitHub, and finally we collected 174 posts and 128 GitHub issues related to architecture decisions. The results show that in AI-based systems development (1) architecture decisions are expressed in six linguistic patterns, among which Solution Proposal and Information Giving are most frequently used, (2) Technology Decision, Component Decision, and Data Decision are the main types of architecture decisions made, (3) Game is the most common application domain among the eighteen application domains identified, (4) the dominant quality attribute considered in architecture decision-making is Performance, and (5) the main limitations and challenges encountered by practitioners in making architecture decisions are Design Issues and Data Issues. Our results suggest that the limitations and challenges when making architecture decisions in AI-based systems development are highly specific to the characteristics of AI-based systems and are mainly of technical nature, which need to be properly confronted.},
  url={https://ieeexplore.ieee.org/abstract/document/10123549},
}

@inproceedings{wang2023app,
  abbr={APSEC},
  bibtex_show={true},
  author={Wang*, Chong and Liu*, Tianyang and Liang, Peng and Daneva, Maya and van Sinderen, Marten},
  booktitle={Proceedings of APSEC},
  title={The Role of User Reviews in App Updates: A Preliminary Investigation on App Release Notes}, 
  year={2021},
  pages={520-525},
  abstract={Release planning for mobile apps has recently become an area of active research. Prior research in this area concentrated on the analysis of release notes and on tracking user reviews to support app evolution with issue trackers. However, little is known about the impact of user reviews on the evolution of mobile apps. Our work explores the role of user reviews in app updates based on release notes. For this purpose, we collected user reviews and release notes of Spotify, the number one app in the 'Music' category in Apple App Store, as the research data. Then, we manually removed non-informative parts of each release note, and manually determined the relevance of the app reviews with respect to the release notes. We did this by using Word2Vec calculation techniques based on the top 80 app release notes with the highest similarities. Our empirical results show that more than 60% of the matched reviews are actually irrelevant to the corresponding release notes. When zooming in at these relevant user reviews, we found that around half of them were posted before the new release and referred to requests, suggestions, and complaints. Whereas, the other half of the relevant user reviews were posted after updating the apps and concentrated more on bug reports and praise.},
  url={https://ieeexplore.ieee.org/abstract/document/9712100},
}